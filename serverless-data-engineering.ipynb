{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless Data Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requirements.txt in venv\n",
    "#Follow the code below in Cloud9 terminal \n",
    "\n",
    "$cd producer\n",
    "$source venv/bin/activate\n",
    "$cd producer\n",
    "$pip install -r ../requirements.txt --target ..\n",
    "\n",
    "\n",
    "ikp3db==1.1.4\n",
    "python-json-logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Producer Lambda\n",
    "> Producer lambda reads the table from Dynamo DB and sends messages to SQS. Before deploying producer lambda, a table should be built in advance. Here we built a table that has pronouns for each gender (he, him, his, her, she, hers) to explore gender inequality in language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "from botocore.vendored import requests\n",
    "\n",
    "DYNAMODB = boto3.resource('dynamodb')\n",
    "TABLE = \"fang\"\n",
    "QUEUE = \"producer\"\n",
    "SQS = boto3.client(\"sqs\")\n",
    "\n",
    "#SETUP LOGGING\n",
    "import logging\n",
    "from pythonjsonlogger import jsonlogger\n",
    "\n",
    "LOG = logging.getLogger()\n",
    "LOG.setLevel(logging.INFO)\n",
    "logHandler = logging.StreamHandler()\n",
    "formatter = jsonlogger.JsonFormatter()\n",
    "logHandler.setFormatter(formatter)\n",
    "LOG.addHandler(logHandler)\n",
    "\n",
    "def scan_table(table):\n",
    "    \"\"\"Scans table and return results\"\"\"\n",
    "\n",
    "    LOG.info(f\"Scanning Table {table}\")\n",
    "    producer_table = DYNAMODB.Table(table)\n",
    "    response = producer_table.scan()\n",
    "    items = response['Items']\n",
    "    LOG.info(f\"Found {len(items)} Items\")\n",
    "    return items\n",
    "\n",
    "def send_sqs_msg(msg, queue_name, delay=0):\n",
    "    \"\"\"Send SQS Message\n",
    "\n",
    "    Expects an SQS queue_name and msg in a dictionary format.\n",
    "    Returns a response dictionary. \n",
    "    \"\"\"\n",
    "\n",
    "    queue_url = SQS.get_queue_url(QueueName=queue_name)[\"QueueUrl\"]\n",
    "    queue_send_log_msg = \"Send message to queue url: %s, with body: %s\" %\\\n",
    "        (queue_url, msg)\n",
    "    LOG.info(queue_send_log_msg)\n",
    "    json_msg = json.dumps(msg)\n",
    "    response = SQS.send_message(\n",
    "        QueueUrl=queue_url,\n",
    "        MessageBody=json_msg,\n",
    "        DelaySeconds=delay)\n",
    "    queue_send_log_msg_resp = \"Message Response: %s for queue url: %s\" %\\\n",
    "        (response, queue_url) \n",
    "    LOG.info(queue_send_log_msg_resp)\n",
    "    return response\n",
    "\n",
    "def send_emissions(table, queue_name):\n",
    "    \"\"\"Send Emissions\"\"\"\n",
    "\n",
    "    items = scan_table(table=table)\n",
    "    for item in items:\n",
    "        LOG.info(f\"Sending item {item} to queue: {queue_name}\")\n",
    "        response = send_sqs_msg(item, queue_name=queue_name)\n",
    "        LOG.debug(response)\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Lambda entrypoint\n",
    "    \"\"\"\n",
    "\n",
    "    extra_logging = {\"table\": TABLE, \"queue\": QUEUE}\n",
    "    LOG.info(f\"event {event}, context {context}\", extra=extra_logging)\n",
    "    send_emissions(table=TABLE, queue_name=QUEUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Consumer Lamda \n",
    "> Every time there is a message, an Event Trigger is fired. Then consumer lambda performs sentiment analysis of the corpus using AWS Comprehend. The result of sentiment analysis is stacked in S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "#SETUP LOGGING\n",
    "import logging\n",
    "from pythonjsonlogger import jsonlogger\n",
    "\n",
    "LOG = logging.getLogger()\n",
    "LOG.setLevel(logging.DEBUG)\n",
    "logHandler = logging.StreamHandler()\n",
    "formatter = jsonlogger.JsonFormatter()\n",
    "logHandler.setFormatter(formatter)\n",
    "LOG.addHandler(logHandler)\n",
    "\n",
    "#S3 BUCKET\n",
    "REGION = \"us-east-1\"\n",
    "\n",
    "queue_name = 'producer'\n",
    "\n",
    "### SQS Utils###\n",
    "def sqs_queue_resource(queue_name):\n",
    "\n",
    "    sqs_resource = boto3.resource('sqs', region_name=REGION)\n",
    "    log_sqs_resource_msg = \"Creating SQS resource conn with qname: [%s] in region: [%s]\" %\\\n",
    "     (queue_name, REGION)\n",
    "    LOG.info(log_sqs_resource_msg)\n",
    "    queue = sqs_resource.get_queue_by_name(QueueName=queue_name)\n",
    "    return queue\n",
    "\n",
    "def sqs_connection():\n",
    "\n",
    "    sqs_client = boto3.client(\"sqs\", region_name=REGION)\n",
    "    log_sqs_client_msg = \"Creating SQS connection in Region: [%s]\" % REGION\n",
    "    LOG.info(log_sqs_client_msg)\n",
    "    return sqs_client\n",
    "\n",
    "def sqs_approximate_count(queue_name):\n",
    "\n",
    "    queue = sqs_queue_resource(queue_name)\n",
    "    attr = queue.attributes\n",
    "    num_message = int(attr['ApproximateNumberOfMessages'])\n",
    "    num_message_not_visible = int(attr['ApproximateNumberOfMessagesNotVisible'])\n",
    "    queue_value = sum([num_message, num_message_not_visible])\n",
    "    sum_msg = \"\"\"'ApproximateNumberOfMessages' and 'ApproximateNumberOfMessagesNotVisible' = *** [%s] *** for QUEUE NAME: [%s]\"\"\" %\\\n",
    "         (queue_value, queue_name)\n",
    "    LOG.info(sum_msg)\n",
    "    return queue_value\n",
    "\n",
    "def delete_sqs_msg(queue_name, receipt_handle):\n",
    "\n",
    "    sqs_client = sqs_connection()\n",
    "    try:\n",
    "        queue_url = sqs_client.get_queue_url(QueueName=queue_name)[\"QueueUrl\"]\n",
    "        delete_log_msg = \"Deleting msg with ReceiptHandle %s\" % receipt_handle\n",
    "        LOG.info(delete_log_msg)\n",
    "        response = sqs_client.delete_message(QueueUrl=queue_url, ReceiptHandle=receipt_handle)\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        exception_msg = \"FAILURE TO DELETE SQS MSG: Queue Name [%s] with error: [%s]\" %\\\n",
    "            (queue_name, error)\n",
    "        LOG.exception(exception_msg)\n",
    "        return None\n",
    "\n",
    "    delete_log_msg_resp = \"Response from delete from queue: %s\" % response\n",
    "    LOG.info(delete_log_msg_resp)\n",
    "    return response\n",
    "\n",
    "def names_to_wikipedia(names):\n",
    "\n",
    "    wikipedia_snippit = []\n",
    "    for name in names:\n",
    "        wikipedia_snippit.append(wikipedia.summary(name, sentences=1))\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            'names':names,\n",
    "            'wikipedia_snippit': wikipedia_snippit\n",
    "        }\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def create_sentiment(row):\n",
    "    \"\"\"Uses AWS Comprehend to Create Sentiments on a DataFrame\"\"\"\n",
    "\n",
    "    LOG.info(f\"Processing {row}\")\n",
    "    comprehend = boto3.client(service_name='comprehend')\n",
    "    payload = comprehend.detect_sentiment(Text=row, LanguageCode='en')\n",
    "    LOG.debug(f\"Found Sentiment: {payload}\")\n",
    "    sentiment = payload['Sentiment']\n",
    "    return sentiment\n",
    "\n",
    "def apply_sentiment(df, column=\"wikipedia_snippit\"):\n",
    "    \"\"\"Uses Pandas Apply to Create Sentiment Analysis\"\"\"\n",
    "\n",
    "    df['Sentiment'] = df[column].apply(create_sentiment)\n",
    "    return df\n",
    "\n",
    "### S3 ###\n",
    "\n",
    "def write_s3(df, bucket, name):\n",
    "    \"\"\"Write S3 Bucket\"\"\"\n",
    "\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    filename = f\"{name}_sentiment.csv\"\n",
    "    res = s3_resource.Object(bucket, filename).\\\n",
    "        put(Body=csv_buffer.getvalue())\n",
    "    LOG.info(f\"result of write to bucket: {bucket} with:\\n {res}\")\n",
    "\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"Entry Point for Lambda\"\"\"\n",
    "\n",
    "    LOG.info(f\"SURVEYJOB LAMBDA, event {event}, context {context}\")\n",
    "    receipt_handle  = event['Records'][0]['receiptHandle'] #sqs message\n",
    "    #'eventSourceARN': 'arn:aws:sqs:us-east-1:561744971673:producer'\n",
    "    event_source_arn = event['Records'][0]['eventSourceARN']\n",
    "\n",
    "    names = [] #Captured from Queue\n",
    "\n",
    "    # Process Queue\n",
    "    for record in event['Records']:\n",
    "        body = json.loads(record['body'])\n",
    "        company_name = body['name']\n",
    "\n",
    "        #Capture for processing\n",
    "        names.append(company_name)\n",
    "\n",
    "        extra_logging = {\"body\": body, \"company_name\":company_name}\n",
    "        LOG.info(f\"SQS CONSUMER LAMBDA, splitting sqs arn with value: {event_source_arn}\",extra=extra_logging)\n",
    "        qname = event_source_arn.split(\":\")[-1]\n",
    "        extra_logging[\"queue\"] = qname\n",
    "        LOG.info(f\"Attemping Deleting SQS receiptHandle {receipt_handle} with queue_name {qname}\", extra=extra_logging)\n",
    "        res = delete_sqs_msg(queue_name=qname, receipt_handle=receipt_handle)\n",
    "        LOG.info(f\"Deleted SQS receipt_handle {receipt_handle} with res {res}\", extra=extra_logging)\n",
    "\n",
    "    # Make Pandas dataframe with wikipedia snippits\n",
    "    LOG.info(f\"Creating dataframe with values: {names}\")\n",
    "    df = names_to_wikipedia(names)\n",
    "\n",
    "    # Perform Sentiment Analysis\n",
    "    df = apply_sentiment(df)\n",
    "    LOG.info(f\"Sentiment from the corpus: {df.to_dict()}\")\n",
    "\n",
    "    # Write result to S3\n",
    "    write_s3(df=df, bucket=\"gendersentiment\", name=names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
